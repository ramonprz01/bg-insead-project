{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumption 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Union\n",
    "import nltk\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_list = ['JobID', 'CleanJobTitle', 'CanonState', 'CanonCounty', 'CanonCity', 'JobText', 'JobDate', 'CanonEmployer', 'Latitude', 'Longitude',\n",
    "              'InternshipFlag', 'IsDuplicate', 'CanonPostalCode', 'CanonYearsOfExperienceLevel', 'BGTSubOcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_smaller_list = ['JobID', 'CleanJobTitle', 'JobText', 'CanonEmployer', 'JobDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={'JobID': np.str, 'CleanJobTitle': np.str, 'JobDomain': np.str, \n",
    "        'CanonCity': np.str, 'CanonCountry': np.str, 'CanonState': np.str, \n",
    "        'JobText': np.str, 'JobURL': np.str, 'PostingHTML': np.float64, \n",
    "        'Source': np.str, 'JobReferenceID': np.str, 'Email': np.str, \n",
    "        'CanonEmployer': np.str, 'Latitude': np.str, 'Longitude': np.str, \n",
    "        'CanonIntermediary': np.str, 'Telephone': np.str, 'CanonJobTitle': 'object', \n",
    "        'CanonCounty': np.str, 'DivisionCode': np.float64, 'MSA': np.str, 'LMA': np.str,\n",
    "        'InternshipFlag': np.str, 'ConsolidatedONET': np.float64, 'CanonCertification': np.str, \n",
    "        'CanonSkillClusters': np.str, 'CanonSkills': np.str, 'IsDuplicate': np.str, \n",
    "        'IsDuplicateOf': np.float64, 'CanonMaximumDegree': np.str, 'CanonMinimumDegree': np.str, \n",
    "        'CanonOtherDegrees': np.str, 'CanonPreferredDegrees': np.str,\n",
    "        'CanonRequiredDegrees': np.str, 'CIPCode': np.str, 'StandardMajor': np.str, \n",
    "        'MaxExperience': np.float64, 'MinExperience': np.float64, 'ConsolidatedInferredNAICS': np.float64, \n",
    "        'BGTOcc': np.str, 'MaxAnnualSalary': np.float64, 'MaxHourlySalary': np.float64, \n",
    "        'MinAnnualSalary': np.float64, 'MinHourlySalary': np.float64, 'YearsOfExperience': np.str, \n",
    "        'CanonJobHours': np.str, 'CanonJobType': np.str, 'CanonPostalCode': np.str, \n",
    "        'CanonYearsOfExperienceCanonLevel': np.str, 'CanonYearsOfExperienceLevel': np.str, \n",
    "        'ConsolidatedTitle': np.str, 'Language': np.str, 'BGTSubOcc': np.str, 'JobDate': np.str,\n",
    "        'ConsolidatedDegreeLevels': np.str, 'MaxDegreeLevel': np.float64, 'MinDegreeLevel': np.float64\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add below the dataset for the week you'd like to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data_18_0806_0812.csv', \n",
    "                 low_memory=False, parse_dates=['JobDate'], usecols=even_smaller_list,\n",
    "                 dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the true memory it is occupying in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out observations without a job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['JobText'].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the `JobText` column, compute the length (in characters) of the job descriptions, and convert clean text into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['clean_text'] = df['JobText'].apply(lambda x: ' '.join(list(filter(None, x.split('\\n')))))\n",
    "df['len_text'] = df['clean_text'].apply(len)\n",
    "df['low_clean'] = df['clean_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_ward = [' will supervise ', 'supervises', ' interns ', ' intern ',\n",
    "             ' guides ', ' mentors ', ' leads ', ' lead ', 'oversees', \n",
    "             'will guide', ' be in charge of ', ' mentor ', 'coaching',\n",
    "             'mentoring', 'coordinating', 'building teams', 'guiding',\n",
    "             'advising', 'setting performance standards', 'resolving conflicts',\n",
    "             'responsibility for outcomes', 'directs', 'appoints', 'instructs',\n",
    "             'recruits', 'manages'\n",
    "]\n",
    "\n",
    "up_ward = [' interns ', ' intern ', 'reports to ', 'report to ', 'answers to', \n",
    "           ' managed by ', ' responds to ', ' directed by ', ' receives guidance ', \n",
    "           ' supervised by ', 'assists', 'supports', 'helps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords approach.\n",
    "\n",
    "1. Identify the keywords above\n",
    "2. Convert the boolean result into integer type\n",
    "3. Replace downward looking keywords with a 3\n",
    "4. Subtract upward looking from downward looking to get the mid\n",
    "5. Replace negative instances of upward looking with a positive 1\n",
    "6. Change 0's to `NaN`\n",
    "7. Create a bucket with labels\n",
    "    - High == downward looking\n",
    "    - mid == mid\n",
    "    - low == upward looking\n",
    "8. Print value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['down_ward'] = df['low_clean'].str.contains(' will supervise | supervises | interns | intern | guides | mentors | leads | lead | oversees | will guide | be in charge of | mentor | coaching | mentoring | coordinating | building teams | guiding | advising | setting performance standards | resolving conflicts | responsibility for outcomes | directs | appoints | instructs | recruits | manages', regex=True)\n",
    "df['upward'] = df['low_clean'].str.contains(' interns | intern | reports to | report to | answers to | managed by | responds to | directed by | receives guidance | supervised by | assists | supports | helps', regex=True)\n",
    "df['upward'] = df['upward'].astype(np.int8)\n",
    "df['down_ward'] = df['down_ward'].astype(np.int8)\n",
    "df['down_ward'] = df['down_ward'].replace(1, 3)\n",
    "df['all_levels'] = (df['down_ward'] - df['upward'])\n",
    "df['all_levels'] = df['all_levels'].replace(-1, 1)\n",
    "df['all_levels'] = df['all_levels'].replace(0, np.nan)\n",
    "labels_dict = {1.0: 'low', 2.0: 'mid', 3.0: 'high'}\n",
    "df['bucket_label'] = df['all_levels'].map(labels_dict)\n",
    "df['bucket_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller sample for testing assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiter out the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_dos = df[df['bucket_label'].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release some memory from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sample of 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos = df_dos.sample(50000)\n",
    "df_dos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep cleaning function. Notice that we want to keep the stopwords in so that is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "    # filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "corp_normalizer = np.vectorize(normalize_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the clean text using all of the cores in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(corp_normalizer, df_dos['clean_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the elements out and assign it back to the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "extract_results = [text for text in results]\n",
    "df_dos['low_clean'] = extract_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['low_clean'] = df_dos['low_clean'].astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['len_text'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get every word instance as a boolean variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Here we iterate throught the list of words\n",
    "for word in down_ward: # and assign the keyword as a variable and a 1 if the word was found\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].str.contains(word) # 0 if not\n",
    "    \n",
    "# Here we iterate throught the list of words\n",
    "for word in up_ward: # and assign the keyword as a variable and a 1 if the word was found\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].str.contains(word) # 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stripped = [w.strip() for w in up_ward]\n",
    "down_stripped = [w.strip() for w in down_ward]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some the amount of keywords in an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['up_instances'] = df_dos.loc[:, up_stripped].sum(axis=1)\n",
    "df_dos['up_instances'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['down_instances'] = df_dos.loc[:, down_stripped].sum(axis=1)\n",
    "df_dos['down_instances'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first 60 characters of the instance where the keywords appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_words(word: str, string: str) -> Union[str, None]:\n",
    "    if word in string:\n",
    "        return string[string.index(word):string.index(word)+60]\n",
    "    \n",
    "for word in up_ward:\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].apply(lambda x: get_words(word, x))\n",
    "    \n",
    "for word in down_ward:\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].apply(lambda x: get_words(word, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Dropbox/Burning Glass/Analysis/company_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_dos.to_csv(path + 'keywords_check_2018_august.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
