{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** We search for keywords in the ads related to hierarchy and separate these into ‘upward looking’ and ‘downward looking’ directions. We classify jobs as ‘high’ in the hierarchy if they contain only downward-looking keywords; we classify jobs as ‘low’ in the hierarchy if they only contain upward-looking keywords; we classify those with at least one instance of each keyword as ‘middle’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "from typing import Union, List\n",
    "import nltk\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this approach we don't need all of the variables so we will use the ones below for simplicity and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_list = ['JobID', 'CleanJobTitle', 'JobText', 'CanonEmployer', 'JobDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the data types before loading them helps us deal with the complexity found in some of the sample files, so we will be passing a dictionary with the column and its respective data type to the `.read_csv()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'JobID': np.str, 'CleanJobTitle': np.str, 'JobDomain': np.str, \n",
    "          'CanonCity': np.str, 'CanonCountry': np.str, 'CanonState': np.str, \n",
    "          'JobText': np.str, 'JobURL': np.str, 'PostingHTML': np.float64, \n",
    "          'Source': np.str, 'JobReferenceID': np.str, 'Email': np.str, \n",
    "          'CanonEmployer': np.str, 'Latitude': np.str, 'Longitude': np.str, \n",
    "          'CanonIntermediary': np.str, 'Telephone': np.str, 'CanonJobTitle': 'object', \n",
    "          'CanonCounty': np.str, 'DivisionCode': np.float64, 'MSA': np.str, 'LMA': np.str,\n",
    "          'InternshipFlag': np.str, 'ConsolidatedONET': np.float64, 'CanonCertification': np.str, \n",
    "          'CanonSkillClusters': np.str, 'CanonSkills': np.str, 'IsDuplicate': np.str, \n",
    "          'IsDuplicateOf': np.float64, 'CanonMaximumDegree': np.str, 'CanonMinimumDegree': np.str, \n",
    "          'CanonOtherDegrees': np.str, 'CanonPreferredDegrees': np.str,\n",
    "          'CanonRequiredDegrees': np.str, 'CIPCode': np.str, 'StandardMajor': np.str, \n",
    "          'MaxExperience': np.float64, 'MinExperience': np.float64, 'ConsolidatedInferredNAICS': np.float64, \n",
    "          'BGTOcc': np.str, 'MaxAnnualSalary': np.float64, 'MaxHourlySalary': np.float64, \n",
    "          'MinAnnualSalary': np.float64, 'MinHourlySalary': np.float64, 'YearsOfExperience': np.str, \n",
    "          'CanonJobHours': np.str, 'CanonJobType': np.str, 'CanonPostalCode': np.str, \n",
    "          'CanonYearsOfExperienceCanonLevel': np.str, 'CanonYearsOfExperienceLevel': np.str, \n",
    "          'ConsolidatedTitle': np.str, 'Language': np.str, 'BGTSubOcc': np.str, 'JobDate': np.str,\n",
    "          'ConsolidatedDegreeLevels': np.str, 'MaxDegreeLevel': np.float64, 'MinDegreeLevel': np.float64\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add below the path to the dataset and the dataset for the week you'd like to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/LaCie SSD/bgdata/data_18/'\n",
    "dataset = 'data_18_0806_0812.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(path + dataset, # combine the two above\n",
    "                 low_memory=False, # since these are large files we need more memory\n",
    "                 parse_dates=['JobDate'], # parse the dates for simplicity\n",
    "                 usecols=small_list, # use our small list of vars\n",
    "                 dtype=dtypes # assign the data types\n",
    "                ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to check the true memory the dataset is occupying in your computer, use the following line of code. You will also get information regarding missing values, shape of the dataframe, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a different perspective of the missing values, calculate the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out observations without a job description since we are not interested in those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['JobText'].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the `JobText` column, compute the length (by characters) of the job descriptions, and convert the `clean_text` column to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['clean_text'] = df['JobText'].apply(lambda x: ' '.join(list(filter(None, x.split('\\n')))))\n",
    "df['len_text'] = df['clean_text'].apply(len)\n",
    "df['low_clean'] = df['clean_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the keywords for our first approach in two separate lists, one for downward looking and for upward looking words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_ward = [' will supervise ', 'supervises', ' interns ', ' intern ',\n",
    "             ' guides ', ' mentors ', ' leads ', ' lead ', 'oversees', \n",
    "             'will guide', ' be in charge of ', ' mentor ', 'coaching',\n",
    "             'mentoring', 'coordinating', 'building teams', 'guiding',\n",
    "             'advising', 'setting performance standards', 'resolving conflicts',\n",
    "             'responsibility for outcomes', 'directs', 'appoints', 'instructs',\n",
    "             'recruits', 'manages'\n",
    "]\n",
    "\n",
    "up_ward = [' interns ', ' intern ', 'reports to ', 'report to ', 'answers to', \n",
    "           ' managed by ', ' responds to ', ' directed by ', ' receives guidance ', \n",
    "           ' supervised by ', 'assists', 'supports', 'helps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords approach.\n",
    "\n",
    "1. Identify the keywords above\n",
    "2. Convert the boolean result into integer type\n",
    "3. Replace downward looking keywords with a 3\n",
    "4. Subtract upward looking from downward looking to get the difference (mid-level keyword match)\n",
    "5. Replace negative instances of upward looking with a positive 1\n",
    "6. Change 0's to `NaN`'s\n",
    "7. Create a bucket with labels\n",
    "    - High == downward looking\n",
    "    - mid == mid\n",
    "    - low == upward looking\n",
    "8. Print value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# the two lines below check for first instance of a keyword OR the next OR the next ...\n",
    "df['down_ward'] = df['low_clean'].str.contains(' will supervise | supervises | interns | intern | guides | mentors | leads | lead | oversees | will guide | be in charge of | mentor | coaching | mentoring | coordinating | building teams | guiding | advising | setting performance standards | resolving conflicts | responsibility for outcomes | directs | appoints | instructs | recruits | manages', regex=True)\n",
    "df['upward'] = df['low_clean'].str.contains(' interns | intern | reports to | report to | answers to | managed by | responds to | directed by | receives guidance | supervised by | assists | supports | helps', regex=True)\n",
    "\n",
    "# change type\n",
    "df['upward'] = df['upward'].astype(np.int8)\n",
    "df['down_ward'] = df['down_ward'].astype(np.int8)\n",
    "\n",
    "# replace nums\n",
    "df['down_ward'] = df['down_ward'].replace(1, 3)\n",
    "\n",
    "# create mid\n",
    "df['all_levels'] = (df['down_ward'] - df['upward'])\n",
    "\n",
    "# replace negative values and 0s\n",
    "df['all_levels'] = df['all_levels'].replace(-1, 1)\n",
    "df['all_levels'] = df['all_levels'].replace(0, np.nan)\n",
    "\n",
    "# label the buckets\n",
    "labels_dict = {1.0: 'low', 2.0: 'mid', 3.0: 'high'}\n",
    "df['bucket_label'] = df['all_levels'].map(labels_dict)\n",
    "\n",
    "df['bucket_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller sample for testing assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiter out of the dataset the missing values from our `bucket_label` var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_dos = df[df['bucket_label'].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release some memory from your computer by deleting the df var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sample of 50k. You can adjust this to your needs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos = df_dos.sample(50000)\n",
    "df_dos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep cleaning function. Notice that we want to keep the stopwords in so that is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "    # filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "corp_normalizer = np.vectorize(normalize_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the clean text using all of the cores in your computer. Notice that we pass in the array as a numpy array for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(corp_normalizer, df_dos['clean_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the elements out and assign them back to the same variable. `.map()` is a lazy evaluator so until we call what we need it won't give us anything in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "extract_results = [text for text in results]\n",
    "df_dos['low_clean'] = extract_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm results by checking out the first add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above is a numpy array inside a list, let's change that in the dataframe to a string and reasign it to the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['low_clean'] = df_dos['low_clean'].astype(np.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the lenght of a job description in character terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['len_text'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get every word instance as a boolean variable and add it back as a column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Here we iterate throught the list of words\n",
    "for word in down_ward: # and assign the keyword as a variable and a 1 if the word was found\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].str.contains(word) # 0 if not\n",
    "    \n",
    "# Here we iterate throught the list of words\n",
    "for word in up_ward: # and assign the keyword as a variable and a 1 if the word was found\n",
    "    df_dos[word.strip()] = df_dos['low_clean'].str.contains(word) # 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of the keywords without spaces in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stripped = [w.strip() for w in up_ward]\n",
    "down_stripped = [w.strip() for w in down_ward]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum up the amount of keyword appearances in an job description/observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['up_instances'] = df_dos.loc[:, up_stripped].sum(axis=1)\n",
    "df_dos['up_instances'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos['down_instances'] = df_dos.loc[:, down_stripped].sum(axis=1)\n",
    "df_dos['down_instances'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first 60 characters of the instance where the keywords appeared. If you would like to see a bigger portion of the text, update the parameter below.\n",
    "\n",
    "You can run this in two ways and the uncommented one is the fastest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def get_words(word: str, string: str) -> Union[str, None]:\n",
    "#     if word in string:\n",
    "#         return string[string.index(word):string.index(word)+60]\n",
    "    \n",
    "# for word in up_ward:\n",
    "#     df_dos[word.strip()] = df_dos['low_clean'].apply(lambda x: get_words(word, x))\n",
    "    \n",
    "# for word in down_ward:\n",
    "#     df_dos[word.strip()] = df_dos['low_clean'].apply(lambda x: get_words(word, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(word: str, string: str) -> Union[str, None]:\n",
    "    if word in string:\n",
    "        return string[string.index(word):string.index(word) + 60]\n",
    "\n",
    "def get_some_text(list_of_words: List[str], data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    for word in list_of_words:\n",
    "        data[word.strip()] = data[column].apply(lambda x: get_words(word, x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_dos = get_some_text(down_stripped, df_dos, 'low_clean')\n",
    "df_dos = get_some_text(up_stripped, df_dos, 'low_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the memory of your dataset and save it with your desired name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dos.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = 'new_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_dos.to_csv(path + new_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
