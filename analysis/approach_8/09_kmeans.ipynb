{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 Clustering Specific Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re, csv, os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import parallel_backend\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "import nltk\n",
    "import concurrent.futures as cf\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/Dropbox/Burning Glass/Data/companies_76k/filtered_data_16/'\n",
    "path_out = '~/Dropbox/Burning Glass/Analysis/approach_8'\n",
    "num = 16\n",
    "fil_num = '06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['JobID', 'CleanJobTitle', 'CanonCity', 'CanonState', 'CanonPostalCode',\n",
    "             'BGTOcc', 'clean_text', 'EmployerClean', 'JobDate']\n",
    "\n",
    "dtypes={'JobID': np.str, 'CanonJobTitle': np.str, 'EmployerClean': np.str,\n",
    "        'CleanJobTitle': np.str, 'CanonCity': np.str, 'CanonCounty': np.str,\n",
    "        'CanonState': np.str, 'ConsolidatedTitle': np.str, 'BGTOcc': np.str,\n",
    "        'JobDate': np.str, 'CanonPostalCode': np.str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['Assistant Manager', 'Deputy Manager', 'Manager', 'Senior Manager', 'General Manager', 'Assistant Director',\n",
    "             'Deputy Director', 'Director', 'Senior Director', 'Deputy Vice President', 'Vice President', 'Senior Vice President',\n",
    "             'President', 'Chief']\n",
    "to_remove_low = [word.lower() for word in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_titles(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in to_remove_low]\n",
    "    clean = ' '.join(filtered_tokens)\n",
    "    return clean\n",
    "\n",
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "    # filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "corp_normalizer = np.vectorize(normalize_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob('random_data/rand*.csv')\n",
    "# files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# def get_files(file):\n",
    "#     return pd.read_csv(file, dtype=dtypes, \n",
    "#                      usecols=best_list, parse_dates=['JobDate'], low_memory=False)\n",
    "\n",
    "\n",
    "# with cf.ThreadPoolExecutor() as executor:\n",
    "#     results = executor.map(get_files, files)\n",
    "    \n",
    "# df = pd.concat(results)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf = dd.read_csv(os.path.join(path, 'da*.csv'), \n",
    "#                  engine='python',\n",
    "#                  dtype=dtypes,\n",
    "#                  assume_missing=True,\n",
    "#                  error_bad_lines=False,\n",
    "#                  blocksize=None,\n",
    "#                  usecols=col_names,\n",
    "#                 )\n",
    "# ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf1 = ddf.map_partitions(lambda data: data.drop_duplicates(subset='CleanJobTitle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CanonCity</th>\n",
       "      <th>CanonState</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>JobDate</th>\n",
       "      <th>JobID</th>\n",
       "      <th>CanonPostalCode</th>\n",
       "      <th>BGTOcc</th>\n",
       "      <th>EmployerClean</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>Visual Merchandiser</td>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>38018841872</td>\n",
       "      <td>10001</td>\n",
       "      <td>27-1026.92</td>\n",
       "      <td>Ethan Allen</td>\n",
       "      <td>Advertisement    Visual Merchandiser - Part Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>Delivery Driver/Warehouse Teammate</td>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>38019329664</td>\n",
       "      <td>10001</td>\n",
       "      <td>53-3032.00</td>\n",
       "      <td>Worldpac</td>\n",
       "      <td>Delivery Driver/Warehouse Teammate    Company:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Palatka</td>\n",
       "      <td>FL</td>\n",
       "      <td>Phlebotomist Prn</td>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>38020414142</td>\n",
       "      <td>32177</td>\n",
       "      <td>31-9097.00</td>\n",
       "      <td>Hospital Corporation of America</td>\n",
       "      <td>Phlebotomist PRN(    Job Number:  26760-349)  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waco</td>\n",
       "      <td>TX</td>\n",
       "      <td>Cosmetics Central Market Pl 296</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>38021484381</td>\n",
       "      <td>76701</td>\n",
       "      <td>41-2031.00</td>\n",
       "      <td>Belk</td>\n",
       "      <td>Cosmetics Central Tx Market Pl Belk #296    Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sumter</td>\n",
       "      <td>SC</td>\n",
       "      <td>Cosmetics Mall 503</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>38021482934</td>\n",
       "      <td>29150</td>\n",
       "      <td>41-1011.00</td>\n",
       "      <td>Belk</td>\n",
       "      <td>Cosmetics Jessamine Mall Belk #503    Sumter, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CanonCity CanonState                       CleanJobTitle     JobDate  \\\n",
       "0  New York         NY                 Visual Merchandiser  2016-02-06   \n",
       "1  New York         NY  Delivery Driver/Warehouse Teammate  2016-02-06   \n",
       "2   Palatka         FL                    Phlebotomist Prn  2016-02-09   \n",
       "3      Waco         TX     Cosmetics Central Market Pl 296  2016-02-11   \n",
       "4    Sumter         SC                  Cosmetics Mall 503  2016-02-11   \n",
       "\n",
       "         JobID CanonPostalCode      BGTOcc                    EmployerClean  \\\n",
       "0  38018841872           10001  27-1026.92                      Ethan Allen   \n",
       "1  38019329664           10001  53-3032.00                         Worldpac   \n",
       "2  38020414142           32177  31-9097.00  Hospital Corporation of America   \n",
       "3  38021484381           76701  41-2031.00                             Belk   \n",
       "4  38021482934           29150  41-1011.00                             Belk   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Advertisement    Visual Merchandiser - Part Ti...  \n",
       "1  Delivery Driver/Warehouse Teammate    Company:...  \n",
       "2  Phlebotomist PRN(    Job Number:  26760-349)  ...  \n",
       "3  Cosmetics Central Tx Market Pl Belk #296    Wa...  \n",
       "4  Cosmetics Jessamine Mall Belk #503    Sumter, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path + f'data_filtered_{fil_num}.csv', low_memory=False, usecols=col_names, dtype=dtypes).drop_duplicates(subset='CleanJobTitle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.74 s, sys: 223 ms, total: 6.97 s\n",
      "Wall time: 7.56 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CanonCity</th>\n",
       "      <th>CanonState</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>JobDate</th>\n",
       "      <th>JobID</th>\n",
       "      <th>CanonPostalCode</th>\n",
       "      <th>BGTOcc</th>\n",
       "      <th>EmployerClean</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46849</th>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>associate scrum master</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>38023624246</td>\n",
       "      <td>37501</td>\n",
       "      <td>15-1199.95</td>\n",
       "      <td>Express Scripts</td>\n",
       "      <td>Associate Scrum Master    Locations: Memphis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26250</th>\n",
       "      <td>Kennesaw</td>\n",
       "      <td>GA</td>\n",
       "      <td>detailer</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>38021250379</td>\n",
       "      <td>30144</td>\n",
       "      <td>53-7061.00</td>\n",
       "      <td>Carmax</td>\n",
       "      <td>CARMAX Detailer in Kennesaw, Georgia    Automo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>Reidsville</td>\n",
       "      <td>NC</td>\n",
       "      <td>cosmetics mall 31</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>38021483110</td>\n",
       "      <td>27320</td>\n",
       "      <td>41-1011.00</td>\n",
       "      <td>Belk</td>\n",
       "      <td>Cosmetics Penrose Mall Belk # 31    Reidsville...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43130</th>\n",
       "      <td>Eden Prairie</td>\n",
       "      <td>MN</td>\n",
       "      <td>supervalu inc - front end developer</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>38024197371</td>\n",
       "      <td>55343</td>\n",
       "      <td>15-1134.92</td>\n",
       "      <td>SuperValu</td>\n",
       "      <td>SUPERVALU Inc SUPERVALU Inc - Front End Develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>Sterling</td>\n",
       "      <td>VA</td>\n",
       "      <td>permit expeditor</td>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>38019110077</td>\n",
       "      <td>20163</td>\n",
       "      <td>43-5061.00</td>\n",
       "      <td>Bohler Engineering</td>\n",
       "      <td>Permit Expeditor          Permit Expeditor    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CanonCity CanonState                        CleanJobTitle  \\\n",
       "46849       Memphis         TN               associate scrum master   \n",
       "26250      Kennesaw         GA                             detailer   \n",
       "10250    Reidsville         NC                    cosmetics mall 31   \n",
       "43130  Eden Prairie         MN  supervalu inc - front end developer   \n",
       "2046       Sterling         VA                     permit expeditor   \n",
       "\n",
       "          JobDate        JobID CanonPostalCode      BGTOcc  \\\n",
       "46849  2016-02-16  38023624246           37501  15-1199.95   \n",
       "26250  2016-02-11  38021250379           30144  53-7061.00   \n",
       "10250  2016-02-11  38021483110           27320  41-1011.00   \n",
       "43130  2016-02-17  38024197371           55343  15-1134.92   \n",
       "2046   2016-02-06  38019110077           20163  43-5061.00   \n",
       "\n",
       "            EmployerClean                                         clean_text  \n",
       "46849     Express Scripts  Associate Scrum Master    Locations: Memphis, ...  \n",
       "26250              Carmax  CARMAX Detailer in Kennesaw, Georgia    Automo...  \n",
       "10250                Belk  Cosmetics Penrose Mall Belk # 31    Reidsville...  \n",
       "43130           SuperValu  SUPERVALU Inc SUPERVALU Inc - Front End Develo...  \n",
       "2046   Bohler Engineering  Permit Expeditor          Permit Expeditor    ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, y_test = train_test_split(df, test_size=.2, random_state=42, shuffle=True)\n",
    "\n",
    "X_train_2, y_test_2 = X_train.copy(), y_test.copy()\n",
    "\n",
    "# %%time\n",
    "\n",
    "# X_train_2['CleanJobTitle'] = X_train_2['CleanJobTitle'].str.lower().apply(lambda x: remove_titles(x, to_remove_low))\n",
    "# y_test_2['CleanJobTitle'] = y_test_2['CleanJobTitle'].str.lower().apply(lambda x: remove_titles(x, to_remove_low))\n",
    "# X_train_2.head(10)\n",
    "\n",
    "rm_titles = np.vectorize(remove_titles)\n",
    "X_train_2['CleanJobTitle'] = rm_titles(X_train_2['CleanJobTitle'].str.lower().values)\n",
    "y_test_2['CleanJobTitle'] = rm_titles(y_test_2['CleanJobTitle'].str.lower().values)\n",
    "y_test_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([\n",
    "    ('normalizer', FunctionTransformer(corp_normalizer)),\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 1), min_df=10, max_df=.85))\n",
    "])\n",
    "\n",
    "km_pipe = Pipeline([\n",
    "    ('km', KMeans(n_clusters=500, # how many clusters do we want\n",
    "            max_iter=1000, # reshuffle each centroid x number of times\n",
    "            n_init=15, # that x num of times can be set here\n",
    "            random_state=42,\n",
    "            n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessing),\n",
    "    ('km_model', km_pipe)\n",
    "])\n",
    "\n",
    "##################################################\n",
    "\n",
    "preprocessing2 = Pipeline([\n",
    "    ('normalizer2', FunctionTransformer(corp_normalizer)),\n",
    "    ('vect2', TfidfVectorizer(ngram_range=(1, 1), min_df=10, max_df=.85))\n",
    "])\n",
    "\n",
    "km_pipe2 = Pipeline([\n",
    "    ('km2', KMeans(n_clusters=500, # how many clusters do we want\n",
    "            max_iter=1000, # reshuffle each centroid x number of times\n",
    "            n_init=15, # that x num of times can be set here\n",
    "            random_state=42,\n",
    "            n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('preprocessor2', preprocessing2),\n",
    "    ('km_model2', km_pipe2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pipe.fit(X_train['CleanJobTitle'].values)\n",
    "    preprocessed_data = pipe.transform(X_train['CleanJobTitle'].values)\n",
    "    pipe2.fit(X_train_2['CleanJobTitle'].values)\n",
    "    preprocessed_data2 = pipe2.transform(X_train_2['CleanJobTitle'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['km_model']['km'].labels_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pipe['km_model']['km'].labels_\n",
    "X_train['clusters'] = clusters\n",
    "X_train['distance'] = preprocessed_data.sum(axis=1).round(2)\n",
    "X_train['dist_dummy'] = np.where(X_train['distance'] < np.percentile(X_train['distance'], 5), 1, 0)\n",
    "\n",
    "test_transformed = pipe['preprocessor'].transform(y_test['CleanJobTitle'].values)\n",
    "test_predict = pipe['km_model'].predict(test_transformed)\n",
    "y_test['predictions'] = test_predict\n",
    "y_test['distance'] = test_transformed.sum(axis=1).round(2)\n",
    "y_test['dist_dummy'] = np.where(y_test['distance'] < np.percentile(y_test['distance'], 5), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2['km_model2']['km2'].labels_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = pipe2['km_model2']['km2'].labels_\n",
    "X_train_2['clusters2'] = clusters2\n",
    "X_train_2['distance2'] = preprocessed_data2.sum(axis=1).round(2)\n",
    "X_train_2['dist_dummy2'] = np.where(X_train_2['distance2'] < np.percentile(X_train_2['distance2'], 5), 1, 0)\n",
    "\n",
    "test_transformed2 = pipe2['preprocessor2'].transform(y_test_2['CleanJobTitle'].values)\n",
    "test_predict2 = pipe2['km_model2'].predict(test_transformed2)\n",
    "y_test_2['predictions2'] = test_predict2\n",
    "y_test_2['distance2'] = test_transformed2.sum(axis=1).round(2)\n",
    "y_test_2['dist_dummy2'] = np.where(y_test_2['distance2'] < np.percentile(y_test_2['distance2'], 5), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back into the main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['clusters2'] = clusters2\n",
    "X_train['distance2'] = preprocessed_data2.sum(axis=1).round(2)\n",
    "X_train['dist_dummy2'] = np.where(X_train_2['distance2'] < np.percentile(X_train_2['distance2'], 5), 1, 0)\n",
    "\n",
    "y_test['predictions2'] = test_predict2\n",
    "y_test['distance2'] = test_transformed2.sum(axis=1).round(2)\n",
    "y_test['dist_dummy2'] = np.where(y_test_2['distance2'] < np.percentile(y_test_2['distance2'], 5), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Trained Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe, f'models/titles_500c_pipe_{num}.pkl');\n",
    "joblib.dump(pipe2, f'models/no_titles_500c_pipe_{num}.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train.to_csv(path_out + '/clustering' + f'/train_titles_in_500c_{num}.csv', index=False)\n",
    "y_test.to_csv(path_out + '/clustering' + f'/test_titles_in_500c_{num}.csv', index=False)\n",
    "X_train_2.to_csv(path_out + '/clustering' + f'/train_titles_out_500c_{num}.csv', index=False)\n",
    "y_test_2.to_csv(path_out + '/clustering' + f'/test_titles_out_500c_{num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
