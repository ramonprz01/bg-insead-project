{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis at Scale on Filtered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help you get answers for the following measures.\n",
    "\n",
    "- **Measure 1:** The new version Relative usage of up-ward & down-ward terms\n",
    "- **Measure 2:** Does the job ad list another job title and tells us who reports to whom?\n",
    "    1. This measure needs to take every keyword above and export the next 7 words in the job ad following it, if any (return NaN otherwise), as a separate column, one for each of the dummies above. \n",
    "- **Measure 3:** How many unique job titles are there in the firm?\n",
    "    1. For each firm-week, firm-year and firm-full-sample, calculate the number of unique (we should have 9 variables for each firm with the number of unique values that either don’t vary within firm, vary within firms each year or vary within firms each week):\n",
    "        - CleanJobTitle\n",
    "        - ConsolidatedJobTitle\n",
    "        - CanonJobTitle\n",
    "- **Measure 4:** Managerial intensity?\n",
    "    1. For each firm-location-week-occupationbelow, calculate the number of job ads with the following job codes:\n",
    "    2. For each firm-location-week-alloccups, calculate also the number of job ads with the following job code:\n",
    "        - Everything starting with ‘11’\n",
    "    3. For each firm-location-week, calculate also the total number of job ads posted by that firm regardless of the occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to keep in mind before running the code in this notebook.\n",
    "\n",
    "- This notebook assumes you already ran the notebook called \"06_dask_get_companies.ipynb\", which gets you the filtered dataset from the cleaned sample. Conversely, you already have access to this dataset\n",
    "- You will be using dask dataframes for distributed computing in your local machine. You can think of these dataframes as lazy pandas (no pun intended)\n",
    "- Depending on how you modify this notebook and decide to use it moving forward, please keep in mind that you will be generating quite a few files at the end of this notebook so make sure to tweak the function at the end of the notebook to adjust the files you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.dataframe as dd, dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re, csv, os\n",
    "import numpy as np\n",
    "from dask import delayed, persist\n",
    "from dask.distributed import Client\n",
    "from glob import glob\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the path where your filtered data lives and put it below in `path` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/ramonperez/Dropbox/Burning Glass/Data/companies_76k/filtered_data_07/'\n",
    "path_out = '/Users/ramonperez/Dropbox/Burning Glass/Analysis/approach_8/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two lists contain the names of the clean variables from the previous steps and the data types we will be using to read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['JobID', 'CleanJobTitle', 'CanonCity', 'CanonState', 'Source', 'Latitude', \n",
    "             'Longitude', 'CanonJobTitle', 'CanonCounty', 'MSA', 'LMA', 'InternshipFlag',\n",
    "             'ConsolidatedONET', 'CanonSkillClusters', 'CanonSkills', 'CanonMinimumDegree',\n",
    "             'CanonRequiredDegrees', 'MinExperience', 'ConsolidatedInferredNAICS', 'BGTOcc',\n",
    "             'YearsOfExperience', 'CanonJobHours', 'CanonJobType', 'CanonPostalCode', \n",
    "             'CanonYearsOfExperienceCanonLevel', 'CanonYearsOfExperienceLevel', 'ConsolidatedTitle',\n",
    "             'Language', 'BGTSubOcc', 'ConsolidatedDegreeLevels', 'MinDegreeLevel', 'EmployerClean',\n",
    "             'clean_text', 'JobDate']\n",
    "\n",
    "dtypes={'CanonSkills': np.str, 'Latitude': np.float32, 'JobID': np.str, 'CanonJobTitle': np.str,\n",
    "        'CanonYearsOfExperienceLevel': np.str, 'Longitude': np.float32, 'CanonJobType': np.str, \n",
    "        'CleanJobTitle': np.str, 'ConsolidatedInferredNAICS': np.str, 'CanonRequiredDegrees': np.str,\n",
    "        'YearsOfExperience': np.str, 'CanonCity': np.str, 'CanonCounty': np.str, 'CanonJobHours': np.str,\n",
    "        'CanonState': np.str, 'ConsolidatedONET': np.str, 'MSA': np.str, 'CanonMinimumDegree': np.str,\n",
    "        'ConsolidatedDegreeLevels': np.str, 'BGTSubOcc': np.str, 'ConsolidatedTitle': np.str,\n",
    "        'CanonSkillClusters': np.str, 'Language': np.str, 'JobDate': np.str,\n",
    "        'MinDegreeLevel': np.str, 'LMA': np.str, 'MinExperience': np.str, 'CanonPostalCode': np.str,\n",
    "        'InternshipFlag': np.bool_, 'Source': np.str, 'BGTOcc': np.str, 'CanonYearsOfExperienceCanonLevel': np.str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting in the below cell we will begin creating a directed acyclical graph using dask. This means that we will be making barely any computations until the very end of the notebook.\n",
    "\n",
    "The snippet below will help us read in the amount files in the directory specified above. Make sure to place the wildcard in the appropriate spot, otherwise you will not be able to read in the data.\n",
    "\n",
    "Parameters used:\n",
    "\n",
    "- `engine='python'`\n",
    "- `dtype=dtypes`: our list of data types above\n",
    "- `assume_missing=True`: Yes, there might be some edge cases of missing values not taken care of in our previous step.\n",
    "- `error_bad_lines=False`: We don't want any bad line in our data so let's allow dask to tell us.\n",
    "- `blocksize=None`: Dask usually tries to read in a small sample of the data and makes inferences as to what data type belongs to what. Because in our case some of the job descriptions have quite large amounts of text per observation, dask won't play it nice with our use case and will most likely misinterpret the commas in some of the values in the `JobText` column. Because of this, we will read in every block without making inferences. Luckily, since in the previous step we created small enough files, even reading the full file in will end up being blazingly fast.\n",
    "- `usecols=col_names`: our list of columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobID</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>CanonCity</th>\n",
       "      <th>CanonState</th>\n",
       "      <th>Source</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>CanonJobTitle</th>\n",
       "      <th>CanonCounty</th>\n",
       "      <th>MSA</th>\n",
       "      <th>LMA</th>\n",
       "      <th>InternshipFlag</th>\n",
       "      <th>ConsolidatedONET</th>\n",
       "      <th>CanonSkillClusters</th>\n",
       "      <th>CanonSkills</th>\n",
       "      <th>CanonMinimumDegree</th>\n",
       "      <th>CanonRequiredDegrees</th>\n",
       "      <th>MinExperience</th>\n",
       "      <th>ConsolidatedInferredNAICS</th>\n",
       "      <th>BGTOcc</th>\n",
       "      <th>YearsOfExperience</th>\n",
       "      <th>CanonJobHours</th>\n",
       "      <th>CanonJobType</th>\n",
       "      <th>CanonPostalCode</th>\n",
       "      <th>CanonYearsOfExperienceCanonLevel</th>\n",
       "      <th>CanonYearsOfExperienceLevel</th>\n",
       "      <th>ConsolidatedTitle</th>\n",
       "      <th>Language</th>\n",
       "      <th>BGTSubOcc</th>\n",
       "      <th>ConsolidatedDegreeLevels</th>\n",
       "      <th>MinDegreeLevel</th>\n",
       "      <th>EmployerClean</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>JobDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=12</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>bool</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 36 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 JobID CleanJobTitle CanonCity CanonState  Source Latitude Longitude CanonJobTitle CanonCounty     MSA     LMA InternshipFlag ConsolidatedONET CanonSkillClusters CanonSkills CanonMinimumDegree CanonRequiredDegrees MinExperience ConsolidatedInferredNAICS  BGTOcc YearsOfExperience CanonJobHours CanonJobType CanonPostalCode CanonYearsOfExperienceCanonLevel CanonYearsOfExperienceLevel ConsolidatedTitle Language BGTSubOcc ConsolidatedDegreeLevels MinDegreeLevel EmployerClean clean_text JobDate\n",
       "npartitions=12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "                object        object    object     object  object  float32   float32        object      object  object  object           bool           object             object      object             object               object        object                    object  object            object        object       object          object                           object                      object            object   object    object                   object         object        object     object  object\n",
       "                   ...           ...       ...        ...     ...      ...       ...           ...         ...     ...     ...            ...              ...                ...         ...                ...                  ...           ...                       ...     ...               ...           ...          ...             ...                              ...                         ...               ...      ...       ...                      ...            ...           ...        ...     ...\n",
       "...                ...           ...       ...        ...     ...      ...       ...           ...         ...     ...     ...            ...              ...                ...         ...                ...                  ...           ...                       ...     ...               ...           ...          ...             ...                              ...                         ...               ...      ...       ...                      ...            ...           ...        ...     ...\n",
       "                   ...           ...       ...        ...     ...      ...       ...           ...         ...     ...     ...            ...              ...                ...         ...                ...                  ...           ...                       ...     ...               ...           ...          ...             ...                              ...                         ...               ...      ...       ...                      ...            ...           ...        ...     ...\n",
       "                   ...           ...       ...        ...     ...      ...       ...           ...         ...     ...     ...            ...              ...                ...         ...                ...                  ...           ...                       ...     ...               ...           ...          ...             ...                              ...                         ...               ...      ...       ...                      ...            ...           ...        ...     ...\n",
       "Dask Name: from-delayed, 36 tasks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_csv(os.path.join(path, 'fil*.csv'), \n",
    "                 engine='python',\n",
    "                 dtype=dtypes,\n",
    "                 assume_missing=True,\n",
    "                 error_bad_lines=False,\n",
    "                 blocksize=None,\n",
    "                 usecols=col_names,\n",
    "                )\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when checking the `.head()` or `.tail()` of the datasets you will be working with. Depending on how much data you are trying to view, especially if it doesn't fit into memory, this could take anywhere between 2 minutes to an hour or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ddf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Measure 1:** The new version Relative usage of up-ward & down-ward terms\n",
    "\n",
    "The two lines below check for first instance of a keyword OR the next OR the next, and so forth. Notice the space in between the pipes (`|`), this is to tell python that, for example, the word `intern` should not be one that is part of `international` but rather its own entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "downward = ddf['clean_text'].str.lower().str.contains(' will supervise | supervising | guiding | mentoring | leading | lead | overseeing | will guide | be in charge of | mentor | coaching | mentoring | coordinating | building teams | build team | guiding | advising | setting performance standard | sets performance standard | resolving conflict | resolves conflict | responsibility for outcomes | responsible for outcomes | directing | appointing | instructing | recruiting | managing | approve | approving | assign | assigning | delegate | delegating | control | controlling | review | reviewing | arbitrate | arbitrating | command | commanding | govern | governing ', regex=True)\n",
    "upward = ddf['clean_text'].str.lower().str.contains(' reports to | report to | reporting to | answers to | answer to | managed by | responds to | respond to | directed by | receives guidance | receive guidance | supervised by | assists | assist | support | supports | supporting | helps | help | helping ', regex=True)\n",
    "\n",
    "ddf0 = ddf.assign(downward=downward, upward=upward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create two lists with the words above and use them to create the dummies for our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_words = [' will supervise ', ' supervising ', ' guiding ', ' mentoring ', ' leading ',\n",
    "              ' lead ', ' overseeing ', ' will guide ', ' be in charge of ', ' mentor ', \n",
    "              ' coaching ', ' mentoring ', ' coordinating ', ' building teams ', ' build team ', \n",
    "              ' guiding ', ' advising ', ' setting performance standard ', ' sets performance standard ',\n",
    "              ' resolving conflict ', ' resolves conflict ', ' responsibility for outcomes ', \n",
    "              ' responsible for outcomes ', ' directing ', ' appointing ', ' instructing ',\n",
    "              ' recruiting ', ' managing ', ' approve ', ' approving ', ' assign ', ' assigning ',\n",
    "              ' delegate ', ' delegating ', ' control ', ' controlling ', ' review ', ' reviewing ',\n",
    "              ' arbitrate ', ' arbitrating ', ' command ', ' commanding ', ' govern ', ' governing ']\n",
    "\n",
    "up_words = [' reports to ', ' report to ', ' reporting to ', ' answers to ', ' answer to ', \n",
    "            ' managed by ', ' responds to ', ' respond to ', ' directed by ', ' receives guidance ',\n",
    "            ' receive guidance ', ' supervised by ', ' assists ', ' assist ', ' support ', \n",
    "            ' supports ', ' supporting ', ' helps ', ' help ', ' helping ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(data: pd.DataFrame, column: str, words: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function will check for the existance of a word in a column of a dataframe,\n",
    "    create a dummy variable for it, and add it to back into the dataframe.\n",
    "    \"\"\"\n",
    "    for word in words: # and assign the keyword as a variable and a 1 if the word was found\n",
    "        data[word.strip()] = data[column].str.lower().str.contains(word)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has a very useful function called `.map_partitions()` that applies a function to each partition of the dask dataframe while treating these partitions as pandas dataframes. We pass in our function and function parameters without parentheses and without calling anything for the data argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf1 = ddf0.map_partitions(get_indicators, column='clean_text', words=down_words)\n",
    "ddf2 = ddf1.map_partitions(get_indicators, column='clean_text', words=up_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first clean the list of words above so that we can add them to our dataframe as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stripped = [w.strip() for w in up_words]\n",
    "down_stripped = [w.strip() for w in down_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then sum up the appearances of both sets of columns to get a sence of how many of these kewords were spotted in a job description. We will then assign the new arrays back into our dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_instances = ddf2.loc[:, up_stripped].sum(axis=1)\n",
    "down_instances = ddf2.loc[:, down_stripped].sum(axis=1)\n",
    "ddf3 = ddf2.assign(up_instances=up_instances, down_instances=down_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(word: str, string: str, num_chars: int=60) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    This function will retrieve the set of characters following a keywords that\n",
    "    has been spotted in a piece of string. The defaul number of characters is 60.\n",
    "    \"\"\"\n",
    "    \n",
    "    if word in string:\n",
    "        return string[string.index(word):string.index(word) + num_chars]\n",
    "\n",
    "def get_some_text(data: pd.DataFrame, column: str, list_of_words: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function extends the function get_words by adding the set of characters detected back into\n",
    "    its respective column as a piece of string.\n",
    "    \"\"\"\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        data[word.strip()] = data[column].apply(lambda x: get_words(word, x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now map our functions above to our dataframe partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf4 = ddf3.map_partitions(get_some_text, column='clean_text', list_of_words=down_words)\n",
    "ddf5 = ddf4.map_partitions(get_some_text, column='clean_text', list_of_words=up_words)\n",
    "# ddf5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meassure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique job titles are there in the firm?\n",
    "1. For each firm-week, firm-year and firm-full-sample, calculate the number of unique (we should have 9 variables for each firm with the number of unique values that either don’t vary within firm, vary within firms each year or vary within firms each week):\n",
    "    - CleanJobTitle\n",
    "    - ConsolidatedJobTitle\n",
    "    - CanonJobTitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this measure we will need to create a couple of additional variables, years and weeks, in order to use them in our `.groupby()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobDate = dd.to_datetime(ddf5['JobDate'])\n",
    "ddf6 = ddf5.assign(JobDate=JobDate)\n",
    "weeks = ddf6['JobDate'].dt.week\n",
    "years = ddf6['JobDate'].dt.year\n",
    "ddf7 = ddf6.assign(weeks=weeks, years=years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove duplicate titles from a specific dataset to then use groupby with this deduplicated version. We will then count the jobs within each of the job title variables above and reset the index to get rid of the three dimensional index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_deduplicated = ddf7.drop_duplicates(subset=['CleanJobTitle'])\n",
    "firm_full_sample = ddf_deduplicated.groupby('EmployerClean')[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()\n",
    "firm_year = ddf_deduplicated.groupby(['EmployerClean', 'years'])[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()\n",
    "firm_week = ddf_deduplicated.groupby(['EmployerClean', 'weeks'])[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check the resulf from the above you can run the computations by uncommenting the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# firm_full_sample, firm_year, firm_week = dask.compute(firm_full_sample, firm_year, firm_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployerClean</th>\n",
       "      <th>weeks</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>ConsolidatedTitle</th>\n",
       "      <th>CanonJobTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Source</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-800-GOT-JUNK?</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-800-GOT-JUNK?</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010Data</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10Th Magnitude</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EmployerClean  weeks  CleanJobTitle  ConsolidatedTitle  CanonJobTitle\n",
       "0         1 Source     14              1                  1              1\n",
       "1  1-800-GOT-JUNK?     14              9                  9              9\n",
       "2  1-800-GOT-JUNK?     25              6                  6              6\n",
       "3         1010Data     14              1                  1              1\n",
       "4   10Th Magnitude     14              2                  2              2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# firm_week.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meassure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managerial intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "For each firm-location-week-occupation, calculate the number of job ads with the managerial job codes available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "occu_condition = ddf7['BGTOcc'].str.startswith('11')\n",
    "managers_dummy_df = ddf7.assign(managerial_occu=occu_condition)\n",
    "managers_only_df = managers_dummy_df[managers_dummy_df['managerial_occu'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers_group1 = managers_only_df.groupby(['EmployerClean', 'CanonState', 'CanonCounty', 'CanonPostalCode', 'weeks', 'BGTOcc'])\n",
    "individual_managers = managers_group1[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "For each firm-location-week-alloccups, calculate also the number of job ads with the following job code\n",
    "- Everything starting with ‘11’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers_group2 = managers_only_df.groupby(['EmployerClean', 'CanonState', 'CanonCounty', 'CanonPostalCode', 'weeks'])\n",
    "all_managers = managers_group2[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "For each firm-location-week, calculate the total number of job ads posted by that firm regardless of the occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_loc_week_group = ddf7.groupby(['EmployerClean', 'CanonState', 'CanonCounty', 'CanonPostalCode', 'weeks'])\n",
    "firm_loc_week_df = firm_loc_week_group[['CleanJobTitle', 'ConsolidatedTitle', 'CanonJobTitle']].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check the result from the above cells you can run the computations by uncommenting the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 34.8 s, total: 4min 5s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# individual_managers, all_managers, firm_loc_week_df = dask.compute(individual_managers, all_managers, firm_loc_week_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployerClean</th>\n",
       "      <th>CanonState</th>\n",
       "      <th>CanonCounty</th>\n",
       "      <th>CanonPostalCode</th>\n",
       "      <th>weeks</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>ConsolidatedTitle</th>\n",
       "      <th>CanonJobTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-800-GOT-JUNK?</td>\n",
       "      <td>IN</td>\n",
       "      <td>Marion</td>\n",
       "      <td>46201</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-800-GOT-JUNK?</td>\n",
       "      <td>UT</td>\n",
       "      <td>Salt Lake</td>\n",
       "      <td>84101</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020 Companies</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90247</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020 Companies</td>\n",
       "      <td>CA</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>91708</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020 Companies</td>\n",
       "      <td>CA</td>\n",
       "      <td>Solano</td>\n",
       "      <td>95687</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EmployerClean CanonState     CanonCounty CanonPostalCode  weeks  \\\n",
       "0  1-800-GOT-JUNK?         IN          Marion           46201     14   \n",
       "1  1-800-GOT-JUNK?         UT       Salt Lake           84101     14   \n",
       "2   2020 Companies         CA     Los Angeles           90247     14   \n",
       "3   2020 Companies         CA  San Bernardino           91708     14   \n",
       "4   2020 Companies         CA          Solano           95687     14   \n",
       "\n",
       "   CleanJobTitle  ConsolidatedTitle  CanonJobTitle  \n",
       "0              1                  1              1  \n",
       "1              1                  1              1  \n",
       "2              1                  1              1  \n",
       "3              1                  1              1  \n",
       "4              1                  1              1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# firm_loc_week_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will help you save a csv file with the following characteristics:\n",
    "- choose between 1 or many datasets for the output of your measure\n",
    "- create a new directory for this output, based on the path provided at the beginning of this notebook\n",
    "- add a name for your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_files(new_dir_name, data, new_file_name, pandas_or_dask=True, partitions=None):\n",
    "    \n",
    "    if not os.path.exists(os.path.join(path_out, new_dir_name)):\n",
    "        os.makedirs(os.path.join(path_out, new_dir_name))\n",
    "\n",
    "    if pandas_or_dask == True:\n",
    "        data.to_csv(os.path.join(path_out, new_dir_name, new_file_name + '.csv'), index=False)\n",
    "    else:\n",
    "        # the following lines of code will take the last dataset, repartition it,\n",
    "        # and save it to the desired location. Notice the wildcard \"*\" below. That is\n",
    "        # the spot Dask will use to number your files starting from 0\n",
    "        (data\n",
    "         .repartition(npartitions=partitions)\n",
    "         .to_csv(os.path.join(path_out, new_dir_name, new_file_name + '*.csv'), index=False)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 41s, sys: 5min 11s, total: 32min 53s\n",
      "Wall time: 35min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_csv_files(new_dir_name='measure_2', data=ddf5,                new_file_name='keywords_',           pandas_or_dask=False, partitions=10)\n",
    "save_csv_files(new_dir_name='measure_3', data=firm_full_sample,    new_file_name='firm_full_sample',    pandas_or_dask=True)\n",
    "save_csv_files(new_dir_name='measure_3', data=firm_year,           new_file_name='firm_year',           pandas_or_dask=True)\n",
    "save_csv_files(new_dir_name='measure_3', data=firm_week,           new_file_name='firm_week',           pandas_or_dask=True)\n",
    "save_csv_files(new_dir_name='measure_4', data=individual_managers, new_file_name='individual_managers', pandas_or_dask=True)\n",
    "save_csv_files(new_dir_name='measure_4', data=all_managers,        new_file_name='all_managers',        pandas_or_dask=True)\n",
    "save_csv_files(new_dir_name='measure_4', data=firm_loc_week_df,    new_file_name='firm_loc_week_df',    pandas_or_dask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
