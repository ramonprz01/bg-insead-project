{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Many Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re, csv, os\n",
    "import numpy as np\n",
    "from dask import delayed, persist\n",
    "from glob import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do before running these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the path to your files to the variable **path** below.\n",
    "\n",
    "For **partitions_out** below think about how many GB you will be cleaning and how many files you will like to have at the end of the cleaning process. A good rule of thumb is to split large files into manageable chunks of 300 to 600 MB for analysis. If you would like to follow this approach, figure out how much data you will be cleaning in MB terms (1 GB = 1000 MB) and divide it by the size in MB terms that you would like your final files to have. For example, 3GB (or 3,000MB) divided by 300MB would amount to 10 partitions.\n",
    "\n",
    "For **partitions_in**, do something somewhat similar than with **partitions_out** but to a much larger scale. If you are cleaning 100 GB of data make about 1000 partitions so that dask can clean faster with very manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test'\n",
    "partitions_in = 10\n",
    "partitions_out = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the variables which I've determined the most useful. Feel free to add or subtract from them before running the cells below. No need to update the `dtypes` dictionary below as it contains all the variables in the BG dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_list = ['JobID', 'CleanJobTitle', 'CanonCity', 'CanonState', 'JobDate', 'JobText', 'Source', 'CanonEmployer',\n",
    "             'Latitude', 'Longitude', 'CanonIntermediary', 'CanonJobTitle', 'CanonCounty', 'DivisionCode', 'MSA', 'LMA',\n",
    "             'InternshipFlag', 'ConsolidatedONET', 'CanonSkillClusters', 'CanonSkills', 'IsDuplicate', 'CanonMinimumDegree', \n",
    "             'CanonRequiredDegrees', 'CIPCode', 'MinExperience', 'ConsolidatedInferredNAICS', 'BGTOcc', 'MaxAnnualSalary',\n",
    "             'MaxHourlySalary', 'MinAnnualSalary', 'MinHourlySalary', 'YearsOfExperience', 'CanonJobHours', 'CanonJobType',\n",
    "             'CanonPostalCode', 'CanonYearsOfExperienceCanonLevel', 'CanonYearsOfExperienceLevel', 'ConsolidatedTitle', \n",
    "             'Language', 'BGTSubOcc', 'ConsolidatedDegreeLevels', 'MaxDegreeLevel', 'MinDegreeLevel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is very messy and dask can't infer correctly all of the variables' data types without taking away the gain of parallelizing the computations, we will import every var with the data type as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={'JobID': np.str, 'CleanJobTitle': np.str, 'JobDomain': np.str, \n",
    "        'CanonCity': np.str, 'CanonCountry': np.str, 'CanonState': np.str, \n",
    "        'JobText': np.str, 'JobURL': np.str, 'PostingHTML': np.str, \n",
    "        'Source': np.str, 'JobReferenceID': np.str, 'Email': np.str, \n",
    "        'CanonEmployer': np.str, 'Latitude': np.str, 'Longitude': np.str, \n",
    "        'CanonIntermediary': np.str, 'Telephone': np.str, 'CanonJobTitle': np.str, \n",
    "        'CanonCounty': np.str, 'DivisionCode': np.str, 'MSA': np.str, 'LMA': np.str,\n",
    "        'InternshipFlag': np.str, 'ConsolidatedONET': np.str, 'CanonCertification': np.str, \n",
    "        'CanonSkillClusters': np.str, 'CanonSkills': np.str, 'IsDuplicate': np.str, \n",
    "        'IsDuplicateOf': np.str, 'CanonMaximumDegree': np.str, 'CanonMinimumDegree': np.str, \n",
    "        'CanonOtherDegrees': np.str, 'CanonPreferredDegrees': np.str,\n",
    "        'CanonRequiredDegrees': np.str, 'CIPCode': np.str, 'StandardMajor': np.str, \n",
    "        'MaxExperience': np.str, 'MinExperience': np.str, 'ConsolidatedInferredNAICS': np.str, \n",
    "        'BGTOcc': np.str, 'MaxAnnualSalary': np.str, 'MaxHourlySalary': np.str, \n",
    "        'MinAnnualSalary': np.str, 'MinHourlySalary': np.str, 'YearsOfExperience': np.str, \n",
    "        'CanonJobHours': np.str, 'CanonJobType': np.str, 'CanonPostalCode': np.str, \n",
    "        'CanonYearsOfExperienceCanonLevel': np.str, 'CanonYearsOfExperienceLevel': np.str, \n",
    "        'ConsolidatedTitle': np.str, 'Language': np.str, 'BGTSubOcc': np.str, 'JobDate': np.str,\n",
    "        'ConsolidatedDegreeLevels': np.str, 'MaxDegreeLevel': np.str, 'MinDegreeLevel': np.str,\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the wildcard in the `os.path.join()` call of your dask dataframe `read_csv` function. That tells Dask to grab all of the files that end with `'.csv'` inside your folder. You can make it more specific by adding more characters before and after the star. For example, `'data_0*.csv'` will grab all CSV files in your folder that start with `data_0` and end with `.csv`.\n",
    "\n",
    "Also notice the we pass in the list of variables and the the dictionary of data types. We also tell dask to assume that there will be missing data with the parameter `assume_missing`. Error bad lines will print the bad lines that dask skips for us.\n",
    "\n",
    "Make sure to add a few letters from the start of your files.\n",
    "\n",
    "Now run everything and wait. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(os.path.join(path, 'da*.csv'), \n",
    "                 engine='python', \n",
    "                 dtype=dtypes,\n",
    "                 assume_missing=True,\n",
    "                 error_bad_lines=False,\n",
    "                 blocksize=None,\n",
    "                 usecols=best_list,\n",
    "                )\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is where we repartition our data\n",
    "ddf00 = ddf.repartition(npartitions=partitions_in)\n",
    "ddf00.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# ddf00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# ddf00.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are missing company names that map to a recruiting agency and because of this\n",
    "# we will identify those observations and fill in the missing valyes in the CanonEmployer\n",
    "# var with \"Recruitment Agency\"\n",
    "EmployerCondition = ((ddf00['CanonEmployer'].isnull()) & (ddf00['CanonIntermediary'].notnull()))\n",
    "EmployerClean = ddf00['CanonEmployer'].where(~EmployerCondition, 'Recruitment Agency')\n",
    "\n",
    "# we will then drop the original variable and add the new one to the dataset using the following methods\n",
    "ddf_clean0 = ddf00.drop('CanonEmployer', axis=1)\n",
    "ddf_clean01 = ddf_clean0.assign(EmployerClean=EmployerClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The following cell will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We have a lot of missing values in this dataset so let's start by calculating those\n",
    "# as a percentage of all of the samples in our datasets\n",
    "# missing_count = ((ddf_clean01.isna().sum() / ddf_clean01.index.size) * 100)\n",
    "# missing_count_pct = missing_count.compute()\n",
    "# missing_count_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now drop the columns with 60% or more missing values\n",
    "cols_to_drop = ['CanonIntermediary', 'DivisionCode', 'CIPCode', 'MaxAnnualSalary', 'MaxHourlySalary', \n",
    "                'MinAnnualSalary', 'MinHourlySalary', 'MaxDegreeLevel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the rows above have more than 60, 70 and 80% of missing values,\n",
    "# we will be getting rid of them with the drop command\n",
    "ddf_clean1 = ddf_clean01.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# since english must be the most common language for the majority of positions in \n",
    "# the USA, we will fill in missing values in that colunm with the en value in the Language var\n",
    "ddf_clean2 = ddf_clean1.fillna({'Language': 'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will get rid of the rows in columns with missing values \n",
    "# between 1 and 10%\n",
    "rows_to_drop = ['CleanJobTitle', 'CanonCity', 'CanonState', 'Source', 'Latitude', 'Longitude', 'CanonCounty',\n",
    "                'MSA', 'LMA', 'ConsolidatedONET', 'CanonSkillClusters', 'BGTOcc', 'CanonPostalCode', 'ConsolidatedTitle',\n",
    "                'BGTSubOcc', 'EmployerClean', 'JobText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the code to drop them\n",
    "ddf_clean3 = ddf_clean2.dropna(subset=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will assign the word \"Unknown\" the remaining columns with missing values\n",
    "# The nice thing about python and many other languages is that we can read the data\n",
    "# and tell it to reassign np.nan to observations containing the word \"Unknown\"\n",
    "# remaining_cols_to_clean = list(missing_count_pct[(missing_count_pct >= 10) & (missing_count_pct < 60)].index)\n",
    "unknown_default_dict = {'CanonJobTitle': 'Unknown', 'CanonMinimumDegree': 'Unknown', 'CanonRequiredDegrees': 'Unknown',\n",
    "                        'MinExperience': 'Unknown', 'ConsolidatedInferredNAICS': 'Unknown', 'YearsOfExperience': 'Unknown',\n",
    "                        'CanonJobHours': 'Unknown', 'CanonJobType': 'Unknown', 'CanonYearsOfExperienceCanonLevel': 'Unknown',\n",
    "                        'CanonYearsOfExperienceLevel': 'Unknown', 'ConsolidatedDegreeLevels': 'Unknown', 'MinDegreeLevel': 'Unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we fill in those missing values\n",
    "ddf_clean4 = ddf_clean3.fillna(unknown_default_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to make sure you don't have any other missing values,\n",
    "# uncomment and run the cell below\n",
    "\n",
    "# print(ddf_clean4.isnull().sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The JobText var is not formatted correctly so we will first clean it\n",
    "# and create a new variable called clean_text\n",
    "clean_text = ddf_clean4.loc[:, 'JobText'].str.strip()\n",
    "# .apply(lambda x: ' '.join(list(filter(None, x.split()))), meta=np.str)\n",
    "\n",
    "# we will then drop the old JobText var\n",
    "ddf_clean5 = ddf_clean4.drop('JobText', axis=1)\n",
    "\n",
    "# Here we reassign the cleaned var back into the dataset\n",
    "ddf_clean6 = ddf_clean5.assign(clean_text=clean_text)\n",
    "\n",
    "# we will now filter out job descriptions that are not written in english\n",
    "english_condition = ddf_clean6['Language'].isin(['en'])\n",
    "ddf_clean7 = ddf_clean6[english_condition]\n",
    "\n",
    "# We will then convert the JobDate var into a date variable\n",
    "# dates = dd.to_datetime(ddf_clean7['JobDate'])\n",
    "# # drop the old one\n",
    "# ddf_clean8 = ddf_clean7.drop('JobDate', axis=1)\n",
    "# # and reassign the new one\n",
    "# ddf_clean9 = ddf_clean8.assign(JobDate=dates)\n",
    "\n",
    "# let's filter out duplicate jobs and then drop that column\n",
    "no_duplicates = (ddf_clean7['IsDuplicate'] == \"FALSE\")\n",
    "ddf_clean8 = ddf_clean7[no_duplicates]\n",
    "ddf_clean9 = ddf_clean8.drop(['IsDuplicate', 'Language'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The following cell will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# using the same folder in your path, we will create a new one for the cleaned data\n",
    "# and save our new files there\n",
    "if not os.path.exists(os.path.join(path, 'clean2')):\n",
    "    os.makedirs(os.path.join(path, 'clean2'))\n",
    "    \n",
    "\n",
    "# the following lines of code will take the last dataset, repartition it,\n",
    "# and save it to the desired location. Notice the wildcard \"*\" below. That is\n",
    "# the spot Dask will use to number your files starting from 0\n",
    "(ddf_clean9\n",
    " .repartition(npartitions=partitions_out)\n",
    " .to_csv(os.path.join(path, 'clean2/', 'data_cleaned_*.csv'), index=False)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
