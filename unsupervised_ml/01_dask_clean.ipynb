{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Many Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re, csv, os\n",
    "import numpy as np\n",
    "from dask import delayed, persist\n",
    "from glob import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do before running these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the path to your files to the variable **path** below.\n",
    "\n",
    "For **partitions_out** below think about how many GB you will be cleaning and how many files you will like to have at the end of the cleaning process. A good rule of thumb is to split large files into manageable chunks of 300 to 600 MB for analysis. If you would like to follow this approach, figure out how much data you will be cleaning in MB terms (1 GB = 1000 MB) and divide it by the size in MB terms that you would like your final files to have. For example, 3GB (or 3,000MB) divided by 300MB would amount to 10 partitions.\n",
    "\n",
    "For **partitions_in**, do something somewhat similar than with **partitions_out** but to a much larger scale. If you are cleaning 100 GB of data make about 1000 partitions so that dask can clean faster with very manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test'\n",
    "partitions_in = 10\n",
    "partitions_out = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the variables which I've determined the most useful. Feel free to add or subtract from them before running the cells below. No need to update the `dtypes` dictionary below as it contains all the variables in the BG dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_list = ['JobID', 'CleanJobTitle', 'CanonCity', 'CanonState', 'JobDate', 'JobText', 'Source', 'CanonEmployer',\n",
    "             'Latitude', 'Longitude', 'CanonIntermediary', 'CanonJobTitle', 'CanonCounty', 'DivisionCode', 'MSA', 'LMA',\n",
    "             'InternshipFlag', 'ConsolidatedONET', 'CanonSkillClusters', 'CanonSkills', 'IsDuplicate', 'CanonMinimumDegree', \n",
    "             'CanonRequiredDegrees', 'CIPCode', 'MinExperience', 'ConsolidatedInferredNAICS', 'BGTOcc', 'MaxAnnualSalary',\n",
    "             'MaxHourlySalary', 'MinAnnualSalary', 'MinHourlySalary', 'YearsOfExperience', 'CanonJobHours', 'CanonJobType',\n",
    "             'CanonPostalCode', 'CanonYearsOfExperienceCanonLevel', 'CanonYearsOfExperienceLevel', 'ConsolidatedTitle', \n",
    "             'Language', 'BGTSubOcc', 'ConsolidatedDegreeLevels', 'MaxDegreeLevel', 'MinDegreeLevel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is very messy and dask can't infer correctly all of the variables' data types without taking away the gain of parallelizing the computations, we will import every var with the data type as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={'JobID': np.str, 'CleanJobTitle': np.str, 'JobDomain': np.str, \n",
    "        'CanonCity': np.str, 'CanonCountry': np.str, 'CanonState': np.str, \n",
    "        'JobText': np.str, 'JobURL': np.str, 'PostingHTML': np.str, \n",
    "        'Source': np.str, 'JobReferenceID': np.str, 'Email': np.str, \n",
    "        'CanonEmployer': np.str, 'Latitude': np.str, 'Longitude': np.str, \n",
    "        'CanonIntermediary': np.str, 'Telephone': np.str, 'CanonJobTitle': np.str, \n",
    "        'CanonCounty': np.str, 'DivisionCode': np.str, 'MSA': np.str, 'LMA': np.str,\n",
    "        'InternshipFlag': np.str, 'ConsolidatedONET': np.str, 'CanonCertification': np.str, \n",
    "        'CanonSkillClusters': np.str, 'CanonSkills': np.str, 'IsDuplicate': np.str, \n",
    "        'IsDuplicateOf': np.str, 'CanonMaximumDegree': np.str, 'CanonMinimumDegree': np.str, \n",
    "        'CanonOtherDegrees': np.str, 'CanonPreferredDegrees': np.str,\n",
    "        'CanonRequiredDegrees': np.str, 'CIPCode': np.str, 'StandardMajor': np.str, \n",
    "        'MaxExperience': np.str, 'MinExperience': np.str, 'ConsolidatedInferredNAICS': np.str, \n",
    "        'BGTOcc': np.str, 'MaxAnnualSalary': np.str, 'MaxHourlySalary': np.str, \n",
    "        'MinAnnualSalary': np.str, 'MinHourlySalary': np.str, 'YearsOfExperience': np.str, \n",
    "        'CanonJobHours': np.str, 'CanonJobType': np.str, 'CanonPostalCode': np.str, \n",
    "        'CanonYearsOfExperienceCanonLevel': np.str, 'CanonYearsOfExperienceLevel': np.str, \n",
    "        'ConsolidatedTitle': np.str, 'Language': np.str, 'BGTSubOcc': np.str, 'JobDate': np.str,\n",
    "        'ConsolidatedDegreeLevels': np.str, 'MaxDegreeLevel': np.str, 'MinDegreeLevel': np.str,\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the wildcard in the `os.path.join()` call of your dask dataframe `read_csv` function. That tells Dask to grab all of the files that end with `'.csv'` inside your folder. You can make it more specific by adding more characters before and after the star. For example, `'data_0*.csv'` will grab all CSV files in your folder that start with `data_0` and end with `.csv`.\n",
    "\n",
    "Also notice the we pass in the list of variables and the the dictionary of data types. We also tell dask to assume that there will be missing data with the parameter `assume_missing`. Error bad lines will print the bad lines that dask skips for us.\n",
    "\n",
    "Make sure to add a few letters from the start of your files.\n",
    "\n",
    "Now run everything and wait. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobID</th>\n",
       "      <th>CleanJobTitle</th>\n",
       "      <th>CanonCity</th>\n",
       "      <th>CanonState</th>\n",
       "      <th>JobDate</th>\n",
       "      <th>JobText</th>\n",
       "      <th>Source</th>\n",
       "      <th>CanonEmployer</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>CanonIntermediary</th>\n",
       "      <th>CanonJobTitle</th>\n",
       "      <th>CanonCounty</th>\n",
       "      <th>DivisionCode</th>\n",
       "      <th>MSA</th>\n",
       "      <th>LMA</th>\n",
       "      <th>InternshipFlag</th>\n",
       "      <th>ConsolidatedONET</th>\n",
       "      <th>CanonSkillClusters</th>\n",
       "      <th>CanonSkills</th>\n",
       "      <th>IsDuplicate</th>\n",
       "      <th>CanonMinimumDegree</th>\n",
       "      <th>CanonRequiredDegrees</th>\n",
       "      <th>CIPCode</th>\n",
       "      <th>MinExperience</th>\n",
       "      <th>ConsolidatedInferredNAICS</th>\n",
       "      <th>BGTOcc</th>\n",
       "      <th>MaxAnnualSalary</th>\n",
       "      <th>MaxHourlySalary</th>\n",
       "      <th>MinAnnualSalary</th>\n",
       "      <th>MinHourlySalary</th>\n",
       "      <th>YearsOfExperience</th>\n",
       "      <th>CanonJobHours</th>\n",
       "      <th>CanonJobType</th>\n",
       "      <th>CanonPostalCode</th>\n",
       "      <th>CanonYearsOfExperienceCanonLevel</th>\n",
       "      <th>CanonYearsOfExperienceLevel</th>\n",
       "      <th>ConsolidatedTitle</th>\n",
       "      <th>Language</th>\n",
       "      <th>BGTSubOcc</th>\n",
       "      <th>ConsolidatedDegreeLevels</th>\n",
       "      <th>MaxDegreeLevel</th>\n",
       "      <th>MinDegreeLevel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 3 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                JobID CleanJobTitle CanonCity CanonState JobDate JobText  Source CanonEmployer Latitude Longitude CanonIntermediary CanonJobTitle CanonCounty DivisionCode     MSA     LMA InternshipFlag ConsolidatedONET CanonSkillClusters CanonSkills IsDuplicate CanonMinimumDegree CanonRequiredDegrees CIPCode MinExperience ConsolidatedInferredNAICS  BGTOcc MaxAnnualSalary MaxHourlySalary MinAnnualSalary MinHourlySalary YearsOfExperience CanonJobHours CanonJobType CanonPostalCode CanonYearsOfExperienceCanonLevel CanonYearsOfExperienceLevel ConsolidatedTitle Language BGTSubOcc ConsolidatedDegreeLevels MaxDegreeLevel MinDegreeLevel\n",
       "npartitions=1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "               object        object    object     object  object  object  object        object   object    object            object        object      object       object  object  object         object           object             object      object      object             object               object  object        object                    object  object          object          object          object          object            object        object       object          object                           object                      object            object   object    object                   object         object         object\n",
       "                  ...           ...       ...        ...     ...     ...     ...           ...      ...       ...               ...           ...         ...          ...     ...     ...            ...              ...                ...         ...         ...                ...                  ...     ...           ...                       ...     ...             ...             ...             ...             ...               ...           ...          ...             ...                              ...                         ...               ...      ...       ...                      ...            ...            ...\n",
       "Dask Name: from-delayed, 3 tasks"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_csv(os.path.join(path, 'da*.csv'), \n",
    "                 engine='python', \n",
    "                 dtype=dtypes,\n",
    "                 assume_missing=True,\n",
    "                 error_bad_lines=False,\n",
    "                 blocksize=None,\n",
    "                 usecols=best_list,\n",
    "                )\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is where we repartition our data\n",
    "ddf00 = ddf.repartition(npartitions=partitions_in)\n",
    "ddf00.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf00.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are missing company names that map to a recruiting agency and because of this\n",
    "# we will identify those observations and fill in the missing valyes in the CanonEmployer\n",
    "# var with \"Recruitment Agency\"\n",
    "EmployerCondition = ((ddf00['CanonEmployer'].isnull()) & (ddf00['CanonIntermediary'].notnull()))\n",
    "EmployerClean = ddf00['CanonEmployer'].where(~EmployerCondition, 'Recruitment Agency')\n",
    "\n",
    "# we will then drop the original variable and add the new one to the dataset using the following methods\n",
    "ddf_clean0 = ddf00.drop('CanonEmployer', axis=1)\n",
    "ddf_clean01 = ddf_clean0.assign(EmployerClean=EmployerClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The following cell will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1min 44s, total: 3min 15s\n",
      "Wall time: 4min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JobID                                0.000000\n",
       "CleanJobTitle                        0.009189\n",
       "CanonCity                            0.468759\n",
       "CanonState                           0.030907\n",
       "JobDate                              0.000000\n",
       "JobText                              0.000000\n",
       "Source                               0.777414\n",
       "Latitude                             0.438270\n",
       "Longitude                            0.438270\n",
       "CanonIntermediary                   89.104791\n",
       "CanonJobTitle                       36.044350\n",
       "CanonCounty                          0.471822\n",
       "DivisionCode                        67.377440\n",
       "MSA                                  2.572260\n",
       "LMA                                  0.700703\n",
       "InternshipFlag                       0.000000\n",
       "ConsolidatedONET                     3.761909\n",
       "CanonSkillClusters                   4.789086\n",
       "CanonSkills                          0.000000\n",
       "IsDuplicate                          0.000000\n",
       "CanonMinimumDegree                  47.867333\n",
       "CanonRequiredDegrees                53.569435\n",
       "CIPCode                             81.628030\n",
       "MinExperience                       54.628772\n",
       "ConsolidatedInferredNAICS           22.961412\n",
       "BGTOcc                               6.836202\n",
       "MaxAnnualSalary                     70.195008\n",
       "MaxHourlySalary                     70.195008\n",
       "MinAnnualSalary                     70.195008\n",
       "MinHourlySalary                     70.195008\n",
       "YearsOfExperience                   43.897566\n",
       "CanonJobHours                       49.533816\n",
       "CanonJobType                        47.912580\n",
       "CanonPostalCode                      0.900903\n",
       "CanonYearsOfExperienceCanonLevel    52.025606\n",
       "CanonYearsOfExperienceLevel         52.025606\n",
       "ConsolidatedTitle                    0.009189\n",
       "Language                             0.000000\n",
       "BGTSubOcc                            6.842745\n",
       "ConsolidatedDegreeLevels            47.867054\n",
       "MaxDegreeLevel                      82.751688\n",
       "MinDegreeLevel                      47.867333\n",
       "EmployerClean                        9.740087\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We have a lot of missing values in this dataset so let's start by calculating those\n",
    "# as a percentage of all of the samples in our datasets\n",
    "missing_count = ((ddf_clean01.isna().sum() / ddf_clean01.index.size) * 100)\n",
    "missing_count_pct = missing_count.compute()\n",
    "missing_count_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CanonIntermediary',\n",
       " 'DivisionCode',\n",
       " 'CIPCode',\n",
       " 'MaxAnnualSalary',\n",
       " 'MaxHourlySalary',\n",
       " 'MinAnnualSalary',\n",
       " 'MinHourlySalary',\n",
       " 'MaxDegreeLevel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will now drop the columns with 60% or more missing values\n",
    "cols_to_drop = list(missing_count_pct[missing_count_pct >= 60].index)\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the rows above have more than 60, 70 and 80% of missing values,\n",
    "# we will be getting rid of them with the drop command\n",
    "ddf_clean1 = ddf_clean01.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# since english must be the most common language for the majority of positions in \n",
    "# the USA, we will fill in missing values in that colunm with the en value in the Language var\n",
    "ddf_clean2 = ddf_clean1.fillna({'Language': 'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CleanJobTitle',\n",
       " 'CanonCity',\n",
       " 'CanonState',\n",
       " 'Source',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'CanonCounty',\n",
       " 'MSA',\n",
       " 'LMA',\n",
       " 'ConsolidatedONET',\n",
       " 'CanonSkillClusters',\n",
       " 'BGTOcc',\n",
       " 'CanonPostalCode',\n",
       " 'ConsolidatedTitle',\n",
       " 'BGTSubOcc',\n",
       " 'EmployerClean']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we will get rid of the rows in columns with missing values \n",
    "# between 1 and 10%\n",
    "rows_to_drop = list(missing_count_pct[(missing_count_pct < 10) & (missing_count_pct > 0)].index)\n",
    "rows_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here is the code to drop them\n",
    "ddf_clean3 = ddf_clean2.dropna(subset=rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CanonJobTitle': 'Unknown',\n",
       " 'CanonMinimumDegree': 'Unknown',\n",
       " 'CanonRequiredDegrees': 'Unknown',\n",
       " 'MinExperience': 'Unknown',\n",
       " 'ConsolidatedInferredNAICS': 'Unknown',\n",
       " 'YearsOfExperience': 'Unknown',\n",
       " 'CanonJobHours': 'Unknown',\n",
       " 'CanonJobType': 'Unknown',\n",
       " 'CanonYearsOfExperienceCanonLevel': 'Unknown',\n",
       " 'CanonYearsOfExperienceLevel': 'Unknown',\n",
       " 'ConsolidatedDegreeLevels': 'Unknown',\n",
       " 'MinDegreeLevel': 'Unknown'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will assign the word \"Unknown\" the remaining columns with missing values\n",
    "# The nice thing about python and many other languages is that we can read the data\n",
    "# and tell it to reassign np.nan to observations containing the word \"Unknown\"\n",
    "remaining_cols_to_clean = list(missing_count_pct[(missing_count_pct >= 10) & (missing_count_pct < 60)].index)\n",
    "unknown_default_dict = dict(map(lambda columnName: (columnName, 'Unknown'), remaining_cols_to_clean))\n",
    "unknown_default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we fill in those missing values\n",
    "ddf_clean4 = ddf_clean3.fillna(unknown_default_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to make sure you don't have any other missing values,\n",
    "# uncomment and run the cell below\n",
    "\n",
    "# print(ddf_clean4.isnull().sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The JobText var is not formatted correctly so we will first clean it\n",
    "# and create a new variable called clean_text\n",
    "clean_text = ddf_clean4.loc[:, 'JobText'].str.strip()\n",
    "# .apply(lambda x: ' '.join(list(filter(None, x.split()))), meta=np.str)\n",
    "\n",
    "# we will then drop the old JobText var\n",
    "ddf_clean5 = ddf_clean4.drop('JobText', axis=1)\n",
    "\n",
    "# Here we reassign the cleaned var back into the dataset\n",
    "ddf_clean6 = ddf_clean5.assign(clean_text=clean_text)\n",
    "\n",
    "# we will now filter out job descriptions that are not written in english\n",
    "english_condition = ddf_clean6['Language'].isin(['en'])\n",
    "ddf_clean7 = ddf_clean6[english_condition]\n",
    "\n",
    "# We will then convert the JobDate var into a date variable\n",
    "# dates = dd.to_datetime(ddf_clean7['JobDate'])\n",
    "# # drop the old one\n",
    "# ddf_clean8 = ddf_clean7.drop('JobDate', axis=1)\n",
    "# # and reassign the new one\n",
    "# ddf_clean9 = ddf_clean8.assign(JobDate=dates)\n",
    "\n",
    "# let's filter out duplicate jobs and then drop that column\n",
    "no_duplicates = (ddf_clean7['IsDuplicate'] == \"FALSE\")\n",
    "ddf_clean8 = ddf_clean7[no_duplicates]\n",
    "ddf_clean9 = ddf_clean8.drop(['IsDuplicate', 'Language'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: The following cell will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 3min 28s, total: 6min 9s\n",
      "Wall time: 7min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test/clean2/data_cleaned_0.csv',\n",
       " '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test/clean2/data_cleaned_1.csv',\n",
       " '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test/clean2/data_cleaned_2.csv',\n",
       " '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test/clean2/data_cleaned_3.csv',\n",
       " '/Volumes/LaCie SSD/bgdata/data_19/some_data/new_test/clean2/data_cleaned_4.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# using the same folder in your path, we will create a new one for the cleaned data\n",
    "# and save our new files there\n",
    "if not os.path.exists(os.path.join(path, 'clean2')):\n",
    "    os.makedirs(os.path.join(path, 'clean2'))\n",
    "    \n",
    "\n",
    "# the following lines of code will take the last dataset, repartition it,\n",
    "# and save it to the desired location. Notice the wildcard \"*\" below. That is\n",
    "# the spot Dask will use to number your files starting from 0\n",
    "(ddf_clean9\n",
    " .repartition(npartitions=partitions_out)\n",
    " .to_csv(os.path.join(path, 'clean2/', 'data_cleaned_*.csv'), index=False)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
